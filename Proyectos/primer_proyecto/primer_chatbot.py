from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.messages  import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
import streamlit as st
from dotenv import load_dotenv
import os

load_dotenv()

# Configuraci√≥n de Streamlit
st.set_page_config(page_title="Chatbot con LangChain", page_icon="ü§ñ")
st.title("Chatbot Basico con LangChain")
st.markdown("Este es un chat bot de ejemplo con streamlit y langchain")

# Configuraci√≥n de LangChain
with st.sidebar:
    st.header("Configuraci√≥n")
    
    # Bot√≥n para iniciar una nueva conversaci√≥n
    if st.button("üóëÔ∏è Nueva conversaci√≥n"):
        # ¬øQu√© necesitas limpiar?
        # ¬øQu√© funci√≥n de Streamlit refresca la p√°gina?
        st.session_state.messages = []
        st.rerun()
    
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.5, 0.1)
    model_name = st.selectbox("Modelo", ["gemini-2.5-flash", "gemini-2.0-flash", "gemini-2.0-flash-exp", "gemini-2.0-flash-2024-06-01"])
    
    # ¬°Nuevo! Personalidad configurable
    personalidad = st.selectbox(
        "Personalidad del Asistente",
        [
            "√ötil y amigable",
            "Profesional y formal", 
            "Casual y relajado",
            "Experto t√©cnico",
            "Creativo y divertido"
        ]
    )

    # ¬øC√≥mo recrear√≠as el modelo con los nuevos par√°metros?
    llm = ChatGoogleGenerativeAI(model=model_name,temperature=temperature , google_api_key=os.getenv("GOOGLE_API_KEY"))

    # Template din√°mico basado en personalidad
    system_messages = {
        "√ötil y amigable": "Eres un asistente √∫til y amigable llamado ChatBot Pro. Responde de manera clara y concisa.",
        "Profesional y formal": "Eres un asistente profesional y formal. Proporciona respuestas precisas y bien estructuradas.",
        "Casual y relajado": "Eres un asistente casual y relajado. Habla de forma natural y amigable, como un buen amigo.",
        "Experto t√©cnico": "Eres un asistente experto t√©cnico. Proporciona respuestas detalladas con precisi√≥n t√©cnica.",
        "Creativo y divertido": "Eres un asistente creativo y divertido. Usa analog√≠as, ejemplos creativos y mant√©n un tono alegre."
    }

# # Crear un prompt
# prompt_template = PromptTemplate(
# input_variables=["mensaje", "historial"],
# template="""Eres un asistente √∫til y amigable llamado ChatBot Pro. 
# Historial de conversaci√≥n:
# {historial}
# 
# Responde de manera clara y concisa a la siguiente pregunta: {mensaje}"""
# )

prompt_template = ChatPromptTemplate.from_messages([
    # Mensaje del sistema - Define la personalidad una sola vez
    ("system", system_messages[personalidad]),
    
    # El historial y mensaje actual - se manejan como texto formateado
    ("human", "Historial de conversaci√≥n:\n{historial}\n\nPregunta actual: {mensaje}"),
    
    # Mensaje del asistente - se maneja como texto formateado
    # ("assistant", "{respuesta}")
])

chain = prompt_template | llm

# Crear un chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar el chat history
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("human"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)

# El historial se sigue formateando como texto
historial_texto = ""
for msg in st.session_state.messages[-10:]:
    if isinstance(msg, HumanMessage):
        historial_texto += f"Usuario: {msg.content}\n"
    elif isinstance(msg, AIMessage):
        historial_texto += f"Asistente: {msg.content}\n"

# Capturar la entrada del usuario
pregunta = st.chat_input("¬øEn qu√© puedo ayudarte?")

if pregunta:
    # mostrar la pregunta del usuario
    with st.chat_message("human"):
        st.markdown(pregunta)
    # Generar la respuesta
    try:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""

            # Magia de streaming 
            for chunk in chain.stream({"mensaje": pregunta, "historial": historial_texto}):
                full_response += chunk.content
                response_placeholder.markdown(full_response + "‚ñå")  # El cursor parpadeante

            response_placeholder.markdown(full_response)

        # Almacenar los mensajes
        st.session_state.messages.append(HumanMessage(content=pregunta))
        st.session_state.messages.append(AIMessage(content=full_response))
    except Exception as e:
        # ¬øQu√© tipo de errores podr√≠an ocurrir aqu√≠?
        st.error(f"Error al generar respuesta: {str(e)}")
        st.info("Verifica que tu API Key de OpenAI est√© configurada correctamente.")